{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets accelerate torch torchvision --quiet\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "#  Load and preprocess dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize text with/\\ / padding and truncation\n",
        "    tokenized = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "train_data = dataset['train'].map(preprocess_function, batched=True)\n",
        "val_data = dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "\n",
        "#  Load T5 Model\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Training Setup\n",
        "\n",
        "batch_size = 4\n",
        "epochs = 1\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training Loop\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1} | Step {step+1}/{len(train_dataloader)} | Loss: {total_loss / (step+1):.4f}\")\n",
        "\n",
        "    # Validation after each epoch\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed. Validation Loss: {val_loss / len(val_dataloader):.4f}\")\n",
        "\n",
        "\n",
        "model.save_pretrained(\"./t5_simplified_model\")\n",
        "tokenizer.save_pretrained(\"./t5_simplified_tokenizer\")\n",
        "\n",
        "print(\"Model and tokenizer saved successfully!\")\n",
        "\n",
        "\n",
        "#Text Generation Example\n",
        "\n",
        "prompt = \"The future of AI is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs['input_ids'], max_length=50, num_beams=3, early_stopping=True)  # Reduce generation length\n",
        "print(\"Generated Text:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFWHZsIE20FC",
        "outputId": "087f043e-11e1-41e1-9f42-4f7cc7dfac3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1 | Step 100/9180 | Loss: 3.1914\n",
            "Epoch 1 | Step 200/9180 | Loss: 1.9694\n",
            "Epoch 1 | Step 300/9180 | Loss: 1.4776\n",
            "Epoch 1 | Step 400/9180 | Loss: 1.1873\n",
            "Epoch 1 | Step 500/9180 | Loss: 1.0030\n",
            "Epoch 1 | Step 600/9180 | Loss: 0.8722\n",
            "Epoch 1 | Step 700/9180 | Loss: 0.7772\n",
            "Epoch 1 | Step 800/9180 | Loss: 0.7032\n",
            "Epoch 1 | Step 900/9180 | Loss: 0.6429\n",
            "Epoch 1 | Step 1000/9180 | Loss: 0.5922\n",
            "Epoch 1 | Step 1100/9180 | Loss: 0.5506\n",
            "Epoch 1 | Step 1200/9180 | Loss: 0.5143\n",
            "Epoch 1 | Step 1300/9180 | Loss: 0.4828\n",
            "Epoch 1 | Step 1400/9180 | Loss: 0.4550\n",
            "Epoch 1 | Step 1500/9180 | Loss: 0.4302\n",
            "Epoch 1 | Step 1600/9180 | Loss: 0.4079\n",
            "Epoch 1 | Step 1700/9180 | Loss: 0.3877\n",
            "Epoch 1 | Step 1800/9180 | Loss: 0.3695\n",
            "Epoch 1 | Step 1900/9180 | Loss: 0.3526\n",
            "Epoch 1 | Step 2000/9180 | Loss: 0.3371\n",
            "Epoch 1 | Step 2100/9180 | Loss: 0.3228\n",
            "Epoch 1 | Step 2200/9180 | Loss: 0.3096\n",
            "Epoch 1 | Step 2300/9180 | Loss: 0.2974\n",
            "Epoch 1 | Step 2400/9180 | Loss: 0.2860\n",
            "Epoch 1 | Step 2500/9180 | Loss: 0.2753\n",
            "Epoch 1 | Step 2600/9180 | Loss: 0.2655\n",
            "Epoch 1 | Step 2700/9180 | Loss: 0.2562\n",
            "Epoch 1 | Step 2800/9180 | Loss: 0.2476\n",
            "Epoch 1 | Step 2900/9180 | Loss: 0.2395\n",
            "Epoch 1 | Step 3000/9180 | Loss: 0.2319\n",
            "Epoch 1 | Step 3100/9180 | Loss: 0.2248\n",
            "Epoch 1 | Step 3200/9180 | Loss: 0.2182\n",
            "Epoch 1 | Step 3300/9180 | Loss: 0.2118\n",
            "Epoch 1 | Step 3400/9180 | Loss: 0.2059\n",
            "Epoch 1 | Step 3500/9180 | Loss: 0.2003\n",
            "Epoch 1 | Step 3600/9180 | Loss: 0.1951\n",
            "Epoch 1 | Step 3700/9180 | Loss: 0.1900\n",
            "Epoch 1 | Step 3800/9180 | Loss: 0.1852\n",
            "Epoch 1 | Step 3900/9180 | Loss: 0.1807\n",
            "Epoch 1 | Step 4000/9180 | Loss: 0.1764\n",
            "Epoch 1 | Step 4100/9180 | Loss: 0.1722\n",
            "Epoch 1 | Step 4200/9180 | Loss: 0.1683\n",
            "Epoch 1 | Step 4300/9180 | Loss: 0.1645\n",
            "Epoch 1 | Step 4400/9180 | Loss: 0.1609\n",
            "Epoch 1 | Step 4500/9180 | Loss: 0.1575\n",
            "Epoch 1 | Step 4600/9180 | Loss: 0.1542\n",
            "Epoch 1 | Step 4700/9180 | Loss: 0.1511\n",
            "Epoch 1 | Step 4800/9180 | Loss: 0.1480\n",
            "Epoch 1 | Step 4900/9180 | Loss: 0.1452\n",
            "Epoch 1 | Step 5000/9180 | Loss: 0.1424\n",
            "Epoch 1 | Step 5100/9180 | Loss: 0.1397\n",
            "Epoch 1 | Step 5200/9180 | Loss: 0.1371\n",
            "Epoch 1 | Step 5300/9180 | Loss: 0.1346\n",
            "Epoch 1 | Step 5400/9180 | Loss: 0.1322\n",
            "Epoch 1 | Step 5500/9180 | Loss: 0.1299\n",
            "Epoch 1 | Step 5600/9180 | Loss: 0.1277\n",
            "Epoch 1 | Step 5700/9180 | Loss: 0.1255\n",
            "Epoch 1 | Step 5800/9180 | Loss: 0.1234\n",
            "Epoch 1 | Step 5900/9180 | Loss: 0.1214\n",
            "Epoch 1 | Step 6000/9180 | Loss: 0.1194\n",
            "Epoch 1 | Step 6100/9180 | Loss: 0.1175\n",
            "Epoch 1 | Step 6200/9180 | Loss: 0.1157\n",
            "Epoch 1 | Step 6300/9180 | Loss: 0.1139\n",
            "Epoch 1 | Step 6400/9180 | Loss: 0.1122\n",
            "Epoch 1 | Step 6500/9180 | Loss: 0.1105\n",
            "Epoch 1 | Step 6600/9180 | Loss: 0.1089\n",
            "Epoch 1 | Step 6700/9180 | Loss: 0.1073\n",
            "Epoch 1 | Step 6800/9180 | Loss: 0.1058\n",
            "Epoch 1 | Step 6900/9180 | Loss: 0.1043\n",
            "Epoch 1 | Step 7000/9180 | Loss: 0.1029\n",
            "Epoch 1 | Step 7100/9180 | Loss: 0.1015\n",
            "Epoch 1 | Step 7200/9180 | Loss: 0.1001\n",
            "Epoch 1 | Step 7300/9180 | Loss: 0.0988\n",
            "Epoch 1 | Step 7400/9180 | Loss: 0.0974\n",
            "Epoch 1 | Step 7500/9180 | Loss: 0.0962\n",
            "Epoch 1 | Step 7600/9180 | Loss: 0.0950\n",
            "Epoch 1 | Step 7700/9180 | Loss: 0.0938\n",
            "Epoch 1 | Step 7800/9180 | Loss: 0.0926\n",
            "Epoch 1 | Step 7900/9180 | Loss: 0.0915\n",
            "Epoch 1 | Step 8000/9180 | Loss: 0.0903\n",
            "Epoch 1 | Step 8100/9180 | Loss: 0.0893\n",
            "Epoch 1 | Step 8200/9180 | Loss: 0.0882\n",
            "Epoch 1 | Step 8300/9180 | Loss: 0.0872\n",
            "Epoch 1 | Step 8400/9180 | Loss: 0.0862\n",
            "Epoch 1 | Step 8500/9180 | Loss: 0.0852\n",
            "Epoch 1 | Step 8600/9180 | Loss: 0.0842\n",
            "Epoch 1 | Step 8700/9180 | Loss: 0.0833\n",
            "Epoch 1 | Step 8800/9180 | Loss: 0.0824\n",
            "Epoch 1 | Step 8900/9180 | Loss: 0.0815\n",
            "Epoch 1 | Step 9000/9180 | Loss: 0.0806\n",
            "Epoch 1 | Step 9100/9180 | Loss: 0.0797\n",
            "Epoch 1 completed. Validation Loss: 0.0001\n",
            "Model and tokenizer saved successfully!\n",
            "Generated Text:\n",
            " The future of AI is\n"
          ]
        }
      ]
    }
  ]
}